import asyncio
from typing import List, Dict
from aiocache import cached, Cache
import aiohttp
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import re

from yandex_cloud_ml_sdk import YCloudML

# üìå –î–∞–Ω–Ω—ã–µ –¥–ª—è API –Ø–Ω–¥–µ–∫—Å–∞ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–∏)
YANDEX_FOLDER_ID = "b1g9uv256jlf1pq349lq"  # ID –ø–∞–ø–∫–∏ –≤ Yandex Cloud
YANDEX_API_KEY = "AQVNya6rksDSvOKJbQdaXiLdVXTFUyF5hMzXkQio"  # –í–∞—à API –∫–ª—é—á

# üìå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
MAX_RESULTS = 3        # –°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –±–µ—Ä–µ–º
MAX_TEXT_LENGTH = 1000 # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞

# üìå –ú–∞–ø–ø–∏–Ω–≥ —è–∑—ã–∫–æ–≤ –∫ –¥–æ–º–µ–Ω–∞–º –Ø–Ω–¥–µ–∫—Å–∞
LANGUAGE_DOMAIN_MAP = {
    "lang_ru": "yandex.ru",
    "lang_tr": "yandex.com.tr",
    "lang_com": "yandex.com"
}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º YandexGPT –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
ycp_sdk = YCloudML(
    folder_id=YANDEX_FOLDER_ID,
    auth=YANDEX_API_KEY,
)
yandex_gpt_model = ycp_sdk.models.completions("yandexgpt")
yandex_gpt_model = yandex_gpt_model.configure(temperature=0.5)


@cached(ttl=300, cache=Cache.MEMORY)  # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ 5 –º–∏–Ω—É—Ç
async def perform_yandex_search(query: str, num_results: int = MAX_RESULTS, languages: List[str] = ["lang_ru"]) -> List[Dict[str, str]]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Yandex Search API v1, –ø–æ–ª—É—á–∞–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –æ—á–∏—â–∞–µ—Ç –∏—Ö –∫–æ–Ω—Ç–µ–Ω—Ç.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º, —Å—Å—ã–ª–∫–æ–π –∏ —Ç–µ–∫—Å—Ç–æ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    
    :param query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
    :param num_results: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
    :param languages: –°–ø–∏—Å–æ–∫ —è–∑—ã–∫–æ–≤—ã—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, ["lang_ru", "lang_en"]).
    :return: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ 'title', 'url', 'text'.
    """
    search_results = []

    async with aiohttp.ClientSession() as session:
        for lang in languages:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω –¥–ª—è —è–∑—ã–∫–∞ (–≤ –¥–∞–Ω–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)
            domain = LANGUAGE_DOMAIN_MAP.get(lang, "yandex.ru")
            if not domain:
                print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —è–∑—ã–∫: {lang}. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º.")
                continue

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            params = {
                "folderid": YANDEX_FOLDER_ID,
                "apikey": YANDEX_API_KEY,
                "query": query,
                "lr": "213",            # ID —Ä–µ–≥–∏–æ–Ω–∞ –¥–ª—è –†–æ—Å—Å–∏–∏ (–ú–æ—Å–∫–≤–∞). –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –Ω—É–∂–Ω—ã–π —Ä–µ–≥–∏–æ–Ω
                "l10n": "ru",           # –Ø–∑—ã–∫ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
                "sortby": "rlv",        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                "filter": "strict",     # –§–∏–ª—å—Ç—Ä —Å–µ–º–µ–π—Å—Ç–≤–∞ —Å–∞–π—Ç–æ–≤
                "groupby": "attr=d.mode=deep.groups-on-page=1.docs-in-group=1",  # –ù–∞—Å—Ç—Ä–æ–µ–Ω–æ –ø–æ–¥ MAX_RESULTS=1
                "maxpassages": "3",
                "page": "0"             # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
            }

            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –∑–∞–ø—Ä–æ—Å–∞
            search_url = f"https://{domain}/search/xml"

            try:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º GET-–∑–∞–ø—Ä–æ—Å –∫ Yandex Search API v1
                async with session.get(search_url, params=params) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Yandex API: —Å—Ç–∞—Ç—É—Å {response.status}")
                        print(f"üîç –¢–µ–∫—Å—Ç –æ—à–∏–±–∫–∏: {error_text[:500]}...")
                        continue

                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞
                    response_text = await response.text()

                    # –ü–∞—Ä—Å–∏–º XML-–æ—Ç–≤–µ—Ç
                    try:
                        root = ET.fromstring(response_text)
                    except ET.ParseError as parse_error:
                        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {parse_error}")
                        print(f"üîç –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç: {response_text[:500]}...")
                        continue

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ XML
                    docs = root.findall(".//doc")
                    if not docs:
                        print("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –Ø–Ω–¥–µ–∫—Å–µ.")
                        continue

                    for doc in docs:
                        url = doc.findtext("url")
                        if url:
                            url = url.strip()  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
                        title = doc.findtext("title", default="–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                        if url and url not in [result["url"] for result in search_results]:
                            search_results.append({"title": title, "url": url})
                            if len(search_results) >= num_results:
                                break

            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞ –∫ Yandex API: {e}")
                continue

            if len(search_results) >= num_results:
                break

    if not search_results:
        print("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –Ø–Ω–¥–µ–∫—Å–µ.")
        return []

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–∫–∞—á–∏–≤–∞–µ–º –∏ –ø–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    tasks = [scrape_page(result["url"]) for result in search_results]
    scraped_results = await asyncio.gather(*tasks)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å—Å—ã–ª–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º
    final_results = []
    for i, scraped in enumerate(scraped_results):
        if scraped["text"]:
            final_results.append({
                "title": search_results[i]["title"],
                "url": search_results[i]["url"],
                "text": scraped["text"]
            })

    return final_results


async def strong_clean_text(text: str) -> str:
    """
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç YandexGPT –¥–ª—è –æ—á–µ–Ω—å —Å–∏–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ –≤—Ö–æ–¥—è—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞.
    –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –æ—á–∏—Å—Ç–∏ —Ç–µ–∫—Å—Ç –æ—Ç HTML-—Ç–µ–≥–æ–≤, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤, –æ—Å—Ç–∞–≤—å —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π, —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
    """
    system_msg = (
        "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –æ—á–∏—Å—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞. –û—á–∏—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –æ—Ç HTML-—Ç–µ–≥–æ–≤, —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤, –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤, –∏–∑–±—ã—Ç–æ—á–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ª—é–±—ã—Ö –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. "
        "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—è—Å–Ω–µ–Ω–∏–π. "
    )
    prompt = f"{system_msg}\n\n–¢–µ–∫—Å—Ç:\n{text}\n\n–ß–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç:"
    try:
        result = await asyncio.to_thread(yandex_gpt_model.run, prompt)
        if hasattr(result, "alternatives") and result.alternatives:
            cleaned = result.alternatives[0].text.strip()
            # –ï—Å–ª–∏ –æ—á–∏—Å—Ç–∫–∞ –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            return cleaned if cleaned else text
        else:
            return text
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∏–ª—å–Ω–æ–π –æ—á–∏—Å—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}")
        return text


async def scrape_page(url: str) -> Dict[str, str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –ø–æ URL, –æ—á–∏—â–∞–µ—Ç HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç.
    
    :param url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞.
    :return: –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ 'title', 'url', 'text'.
    """
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        timeout = aiohttp.ClientTimeout(total=10)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url, headers=headers) as response:
                if response.status != 200:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: —Å—Ç–∞—Ç—É—Å {response.status}")
                    return {"title": "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏", "url": url, "text": ""}
                html = await response.text()

        soup = BeautifulSoup(html, "html.parser")

        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for tag in soup(["script", "style", "meta", "head", "footer", "nav", "aside"]):
            tag.decompose()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title_tag = soup.find('title')
        title = title_tag.get_text(strip=True) if title_tag else "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
        text = soup.get_text(separator="\n", strip=True)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
        if len(text) > MAX_TEXT_LENGTH:
            text = text[:MAX_TEXT_LENGTH] + "..."

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—á–µ–Ω—å —Å–∏–ª—å–Ω—É—é –æ—á–∏—Å—Ç–∫—É —Å –ø–æ–º–æ—â—å—é YandexGPT
        cleaned_text = await strong_clean_text(text)

        return {"title": title, "url": url, "text": cleaned_text}

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ {url}: {e}")
        return {"title": "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏", "url": url, "text": ""}
